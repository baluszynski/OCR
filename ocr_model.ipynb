{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OCR machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_7 (Conv2D)           (None, 11, 8, 32)         320       \n",
      "                                                                 \n",
      " max_pooling2d_4 (MaxPooling  (None, 5, 4, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 3, 2, 64)          18496     \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 384)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 64)                24640     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 255)               16575     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 60,031\n",
      "Trainable params: 60,031\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from matplotlib import pyplot\n",
    "%matplotlib inline\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "\n",
    "height = 13\n",
    "width = 10\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation = 'relu',input_shape=(height, width, 1)))\n",
    "#model.add(MaxPooling2D((2, 2)))\n",
    "#model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(255, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data from MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(train_images, y_train), (test_images, y_test) = mnist.load_data(path=\"mnist.npz\")\n",
    "\n",
    "#plot images\n",
    "for i in range(9):\n",
    "    #define subplot\n",
    "    pyplot.subplot(330 + 1 + i)\n",
    "    #plot raw pixel data\n",
    "    pyplot.imshow(train_images[i], cmap=pyplot.get_cmap('gray'))\n",
    "\n",
    "#show the figure\n",
    "pyplot.show()\n",
    "\n",
    "print('Train: Images=%s, Labels=%s' % (train_images.shape, y_train.shape))\n",
    "print('Test: Images=%s, Labels=%s' % (test_images.shape, y_test.shape))\n",
    "\n",
    "x_train = train_images.reshape(train_images.shape[0], 28, 28, 1)\n",
    "x_test = test_images.reshape(test_images.shape[0], 28, 28, 1)\n",
    "input_shape = (28, 28, 1)\n",
    "x_train = x_train.astype('float32')/255\n",
    "x_test = x_test.astype('float32')/255\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total 416\r\n",
      "drwxrwxr-x. 1 1000 1000 1768 Mar 22 18:13 .\r\n",
      "drwxrwxrwx. 1 root root  150 Mar 23 14:05 ..\r\n",
      "-rw-rw-r--. 1 1000 1000  668 Mar 22 18:13 A.png\r\n",
      "-rw-rw-r--. 1 1000 1000  284 Mar 22 18:13 A_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  681 Mar 22 18:13 B.png\r\n",
      "-rw-rw-r--. 1 1000 1000  281 Mar 22 18:13 B_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  624 Mar 22 18:13 C.png\r\n",
      "-rw-rw-r--. 1 1000 1000  249 Mar 22 18:13 C_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  632 Mar 22 18:13 D.png\r\n",
      "-rw-rw-r--. 1 1000 1000  238 Mar 22 18:13 D_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  557 Mar 22 18:13 E.png\r\n",
      "-rw-rw-r--. 1 1000 1000  197 Mar 22 18:13 E_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  510 Mar 22 18:13 F.png\r\n",
      "-rw-rw-r--. 1 1000 1000  162 Mar 22 18:13 F_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  682 Mar 22 18:13 G.png\r\n",
      "-rw-rw-r--. 1 1000 1000  298 Mar 22 18:13 G_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  494 Mar 22 18:13 H.png\r\n",
      "-rw-rw-r--. 1 1000 1000  148 Mar 22 18:13 H_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  535 Mar 22 18:13 I.png\r\n",
      "-rw-rw-r--. 1 1000 1000  176 Mar 22 18:13 I_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  563 Mar 22 18:13 J.png\r\n",
      "-rw-rw-r--. 1 1000 1000  213 Mar 22 18:13 J_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  653 Mar 22 18:13 K.png\r\n",
      "-rw-rw-r--. 1 1000 1000  272 Mar 22 18:13 K_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  488 Mar 22 18:13 L.png\r\n",
      "-rw-rw-r--. 1 1000 1000  136 Mar 22 18:13 L_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  602 Mar 22 18:13 M.png\r\n",
      "-rw-rw-r--. 1 1000 1000  236 Mar 22 18:13 M_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  576 Mar 22 18:13 N.png\r\n",
      "-rw-rw-r--. 1 1000 1000  211 Mar 22 18:13 N_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  677 Mar 22 18:13 O.png\r\n",
      "-rw-rw-r--. 1 1000 1000  277 Mar 22 18:13 O_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  594 Mar 22 18:13 P.png\r\n",
      "-rw-rw-r--. 1 1000 1000  241 Mar 22 18:13 P_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  719 Mar 22 18:13 Q.png\r\n",
      "-rw-rw-r--. 1 1000 1000  280 Mar 22 18:13 Q_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  646 Mar 22 18:13 R.png\r\n",
      "-rw-rw-r--. 1 1000 1000  260 Mar 22 18:13 R_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  675 Mar 22 18:13 S.png\r\n",
      "-rw-rw-r--. 1 1000 1000  289 Mar 22 18:13 S_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  517 Mar 22 18:13 T.png\r\n",
      "-rw-rw-r--. 1 1000 1000  151 Mar 22 18:13 T_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  553 Mar 22 18:13 U.png\r\n",
      "-rw-rw-r--. 1 1000 1000  184 Mar 22 18:13 U_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  647 Mar 22 18:13 V.png\r\n",
      "-rw-rw-r--. 1 1000 1000  289 Mar 22 18:13 V_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  670 Mar 22 18:13 W.png\r\n",
      "-rw-rw-r--. 1 1000 1000  315 Mar 22 18:13 W_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  682 Mar 22 18:13 X.png\r\n",
      "-rw-rw-r--. 1 1000 1000  297 Mar 22 18:13 X_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  613 Mar 22 18:13 Y.png\r\n",
      "-rw-rw-r--. 1 1000 1000  232 Mar 22 18:13 Y_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  617 Mar 22 18:13 Z.png\r\n",
      "-rw-rw-r--. 1 1000 1000  243 Mar 22 18:13 Z_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  656 Mar 22 18:13 a.png\r\n",
      "-rw-rw-r--. 1 1000 1000  267 Mar 22 18:13 a_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  627 Mar 22 18:13 b.png\r\n",
      "-rw-rw-r--. 1 1000 1000  250 Mar 22 18:13 b_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  594 Mar 22 18:13 c.png\r\n",
      "-rw-rw-r--. 1 1000 1000  232 Mar 22 18:13 c_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  614 Mar 22 18:13 d.png\r\n",
      "-rw-rw-r--. 1 1000 1000  233 Mar 22 18:13 d_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  650 Mar 22 18:13 e.png\r\n",
      "-rw-rw-r--. 1 1000 1000  256 Mar 22 18:13 e_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  569 Mar 22 18:13 f.png\r\n",
      "-rw-rw-r--. 1 1000 1000  204 Mar 22 18:13 f_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  697 Mar 22 18:13 g.png\r\n",
      "-rw-rw-r--. 1 1000 1000  240 Mar 22 18:13 g_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  562 Mar 22 18:13 h.png\r\n",
      "-rw-rw-r--. 1 1000 1000  197 Mar 22 18:13 h_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  550 Mar 22 18:13 i.png\r\n",
      "-rw-rw-r--. 1 1000 1000  187 Mar 22 18:13 i_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  565 Mar 22 18:13 j.png\r\n",
      "-rw-rw-r--. 1 1000 1000  160 Mar 22 18:13 j_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  597 Mar 22 18:13 k.png\r\n",
      "-rw-rw-r--. 1 1000 1000  223 Mar 22 18:13 k_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  524 Mar 22 18:13 l.png\r\n",
      "-rw-rw-r--. 1 1000 1000  175 Mar 22 18:13 l_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  554 Mar 22 18:13 m.png\r\n",
      "-rw-rw-r--. 1 1000 1000  205 Mar 22 18:13 m_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  555 Mar 22 18:13 n.png\r\n",
      "-rw-rw-r--. 1 1000 1000  190 Mar 22 18:13 n_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  637 Mar 22 18:13 o.png\r\n",
      "-rw-rw-r--. 1 1000 1000  241 Mar 22 18:13 o_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  632 Mar 22 18:13 p.png\r\n",
      "-rw-rw-r--. 1 1000 1000  254 Mar 22 18:13 p_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  618 Mar 22 18:13 q.png\r\n",
      "-rw-rw-r--. 1 1000 1000  225 Mar 22 18:13 q_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  525 Mar 22 18:13 r.png\r\n",
      "-rw-rw-r--. 1 1000 1000  173 Mar 22 18:13 r_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  637 Mar 22 18:13 s.png\r\n",
      "-rw-rw-r--. 1 1000 1000  257 Mar 22 18:13 s_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  569 Mar 22 18:13 t.png\r\n",
      "-rw-rw-r--. 1 1000 1000  202 Mar 22 18:13 t_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  525 Mar 22 18:13 u.png\r\n",
      "-rw-rw-r--. 1 1000 1000  176 Mar 22 18:13 u_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  596 Mar 22 18:13 v.png\r\n",
      "-rw-rw-r--. 1 1000 1000  236 Mar 22 18:13 v_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  659 Mar 22 18:13 w.png\r\n",
      "-rw-rw-r--. 1 1000 1000  293 Mar 22 18:13 w_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  654 Mar 22 18:13 x.png\r\n",
      "-rw-rw-r--. 1 1000 1000  263 Mar 22 18:13 x_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  678 Mar 22 18:13 y.png\r\n",
      "-rw-rw-r--. 1 1000 1000  259 Mar 22 18:13 y_result.png\r\n",
      "-rw-rw-r--. 1 1000 1000  611 Mar 22 18:13 z.png\r\n",
      "-rw-rw-r--. 1 1000 1000  206 Mar 22 18:13 z_result.png\r\n"
     ]
    }
   ],
   "source": [
    "!tar -xzf data.tar.gz\n",
    "!ls -al data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 13, 10)\n",
      "Train: Images=(49, 13, 10), Labels=(49,)\n",
      "Valid: Images=(3, 13, 10), Labels=(3,)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.image as mpimg\n",
    "from pathlib import Path\n",
    "\n",
    "def rgb2gray(rgb):\n",
    "    return np.dot(rgb[...,:3], [0.2989, 0.5870, 0.1140])\n",
    "\n",
    "# Path to the data directory\n",
    "data_dir = Path(\"./data/\")\n",
    "\n",
    "# Get list of all the images\n",
    "images_paths = sorted(list(map(str, list(data_dir.glob(\"*_result.png\")))))\n",
    "\n",
    "labels = [ord(image_path.split(os.path.sep)[-1].split(\"_result.png\")[0]) for image_path in images_paths]\n",
    "images = [rgb2gray(mpimg.imread(image_path)) for image_path in images_paths]\n",
    "\n",
    "print(np.array(images).shape)\n",
    "\n",
    "def split_data(images, labels, train_size=0.95, shuffle=True):\n",
    "    # 1. Get the total size of the dataset\n",
    "    size = len(images)\n",
    "    # 2. Make an indices array and shuffle it, if required\n",
    "    indices = np.arange(size)\n",
    "    if shuffle:\n",
    "        np.random.shuffle(indices)\n",
    "    # 3. Get the size of training samples\n",
    "    train_samples = int(size * train_size)\n",
    "    # 4. Split data into training and validation sets\n",
    "    x_train, y_train = images[indices[:train_samples]], labels[indices[:train_samples]]\n",
    "    x_test, y_test = images[indices[train_samples:]], labels[indices[train_samples:]]\n",
    "    return x_train, x_test, y_train, y_test\n",
    "\n",
    "\n",
    "# Splitting data into training and validation sets\n",
    "x_train, x_test, y_train, y_test = split_data(np.array(images), np.array(labels))\n",
    "\n",
    "print('Train: Images=%s, Labels=%s' % (x_train.shape, y_train.shape))\n",
    "print('Valid: Images=%s, Labels=%s' % (x_test.shape, y_test.shape))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2/2 [==============================] - 1s 49ms/step - loss: 5.5429 - accuracy: 0.0000e+00\n",
      "Epoch 2/5\n",
      "2/2 [==============================] - 0s 21ms/step - loss: 5.4986 - accuracy: 0.0000e+00\n",
      "Epoch 3/5\n",
      "2/2 [==============================] - 0s 24ms/step - loss: 5.4546 - accuracy: 0.0204\n",
      "Epoch 4/5\n",
      "2/2 [==============================] - 0s 32ms/step - loss: 5.3961 - accuracy: 0.0204\n",
      "Epoch 5/5\n",
      "2/2 [==============================] - 0s 28ms/step - loss: 5.3161 - accuracy: 0.0204\n",
      "1/1 [==============================] - 0s 226ms/step - loss: 5.6700 - accuracy: 0.0000e+00\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train, y_train, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b\n"
     ]
    }
   ],
   "source": [
    "image_index = 1 #############\n",
    "pyplot.imshow(x_test[image_index].reshape(height, width),cmap='Greys')\n",
    "pyplot.show()\n",
    "predict = x_test[image_index].reshape(height,width)\n",
    "pred = model.predict(x_test[image_index].reshape(1, height, width, 1))\n",
    "print(chr(pred.argmax()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "This regularization is popularly known as weight decay as it forces the weights to decay towards zero (but not exactly zero). This strategy drives the weights closer to the origin by adding the regularization term omega which is defined as:\n",
    "$$\n",
    "cost = loss + \\frac{\\lambda}{2m} \\sum ||\\omega||^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "One of the dimensions in the output is <= 0 due to downsampling in conv2d_11. Consider increasing the input size. Received input shape [None, 1, 1, 64] which would produce output shape with a zero or negative value in a dimension.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [22]\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Conv2D(\u001b[38;5;241m64\u001b[39m, (\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m), activation \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39madd(MaxPooling2D((\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)))\n\u001b[0;32m----> 7\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConv2D\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m64\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrelu\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Flatten())\n\u001b[1;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39madd(Dense(\u001b[38;5;241m64\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m, kernel_regularizer\u001b[38;5;241m=\u001b[39mregularizers\u001b[38;5;241m.\u001b[39ml2(\u001b[38;5;241m0.0001\u001b[39m)))\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/training/tracking/base.py:629\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    628\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 629\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    630\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    631\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_self_setattr_tracking \u001b[38;5;241m=\u001b[39m previous_value  \u001b[38;5;66;03m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py:67\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:  \u001b[38;5;66;03m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m---> 67\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     69\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/layers/convolutional.py:304\u001b[0m, in \u001b[0;36mConv.compute_output_shape\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m tf\u001b[38;5;241m.\u001b[39mTensorShape(\n\u001b[1;32m    300\u001b[0m         input_shape[:batch_rank] \u001b[38;5;241m+\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilters] \u001b[38;5;241m+\u001b[39m\n\u001b[1;32m    301\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_spatial_output_shape(input_shape[batch_rank \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m:]))\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    305\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOne of the dimensions in the output is <= 0 \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    306\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdue to downsampling in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Consider \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    307\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mincreasing the input size. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    308\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReceived input shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m which would produce \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    309\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput shape with a zero or negative value in a \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    310\u001b[0m       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdimension.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: One of the dimensions in the output is <= 0 due to downsampling in conv2d_11. Consider increasing the input size. Received input shape [None, 1, 1, 64] which would produce output shape with a zero or negative value in a dimension."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import regularizers\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation = 'relu',input_shape=(height, width, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(10,activation='softmax', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train,train_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout ???\n",
    "\n",
    "Dropout is another interesting regularization technique. At every iteration, it randomly selects some nodes and removes them along with all of their incoming and outgoing connections so each iteration has a different set of nodes and this results in a different set of outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation = 'relu',input_shape=(height, width, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.add(Dense(10,activation='softmax', kernel_regularizer=regularizers.l2(0.0001)))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train,train_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation\n",
    "\n",
    "The simplest way to reduce overfitting is to increase the size of the training data. In this case, there are a few ways of increasing the size of the training data –rotating the image, flipping, scaling, shifting, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(rotation_range=40,\n",
    "                            width_shift_range=0.2,\n",
    "                            height_shift_range=0.2,\n",
    "                            shear_range=0.2,\n",
    "                            zoom_range=0.2,\n",
    "                            horizontal_flip=True,\n",
    "                            fill_mode='nearest')\n",
    "train_datagen.fit(x_train)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation = 'relu',input_shape=(height, width, 1)))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D((2, 2)))\n",
    "model.add(Conv2D(64, (3,3), activation = 'relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(10,activation='softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train,train_labels, epochs=5)\n",
    "test_loss, test_acc = model.evaluate(x_test, test_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping\n",
    "\n",
    "Early stopping is a type of cross-validation strategy where we keep one part of the training set as the validation set. When we see that the performance on the validation set is getting worse, we immediately stop the training on the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train,train_labels, epochs=5, validation_data =(x_test, test_labels),\n",
    "callbacks = [EarlyStopping(monitor = 'val_accuracy', patience = 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
